{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48370228-fa95-4d13-a694-c0b8bc5e15e1",
   "metadata": {},
   "source": [
    "# Preprocessing Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6e89e1-8c59-442b-96be-79e3fc889fed",
   "metadata": {},
   "source": [
    "#### 1.Clean the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a0daed1-c154-4bfb-94e8-6ebc14cab8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class EnglishDataCleaning:\n",
    "    ''' This class attempts to clean the raw english text by employing multiple cores. This class takes\n",
    "        list of raw english strings as input and using clean_fast method it cleans the english text.\n",
    "        clean_fast method also takes batch_size as parameter to perform cleaning in batches if data\n",
    "        does not fit into the main memeory.\n",
    "        \n",
    "        Note: This method removes stop words.\n",
    "        \n",
    "        '''\n",
    "    def __init__(self, data_dir, save_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        \n",
    "    def clean_fast(self,batch_size,start_batch_no=0):\n",
    "        '''Method handling data in batches.'''\n",
    "        with open('config.json', 'r') as f:\n",
    "            config = json.load(f)\n",
    "        if config[\"Last_batch_cleaned\"] <= len(os.listdir(self.data_dir)):\n",
    "            start_batch_no = config[\"Last_batch_cleaned\"]\n",
    "            list_of_files = os.listdir(self.data_dir)[start_batch_no:start_batch_no+batch_size]\n",
    "\n",
    "            for i in range(len(list_of_files)):\n",
    "                    \n",
    "                print(f\"Loading batch {start_batch_no+1} into memory...\")\n",
    "                data = []\n",
    "                for j in tqdm(list_of_files):\n",
    "                    with open(self.data_dir + f\"/{j}\", 'rb') as f:\n",
    "                        for k in json.load(f):\n",
    "                            for m in k['text'].split('.'):\n",
    "                                data.append(m)\n",
    "                print(f'Loaded batch {start_batch_no+1}.')\n",
    "                print(f'Cleaning batch {start_batch_no+1}')\n",
    "                self.fast_cleaning(start_batch_no,data)\n",
    "                print(f'Cleaning of batch {start_batch_no+1} done.')\n",
    "                print(f'Saving batch {start_batch_no+1}...')\n",
    "                if not os.path.isdir(self.save_dir):\n",
    "                    os.mkdir(self.save_dir)\n",
    "                with open(f'{self.save_dir}/cleaned_batch_{start_batch_no+1}.pkl', 'wb') as f:\n",
    "                    pickle.dump(data, f)\n",
    "                print(f'Batch no {start_batch_no+1} saved successfully!')\n",
    "            config[\"Last_batch_cleaned\"] = start_batch_no + batch_size\n",
    "            with open('config.json', 'w') as f:\n",
    "                json.dump(config, f)\n",
    "                \n",
    "            \n",
    "        else:\n",
    "            print(\"Error! start_batch_no should be >= 1\")\n",
    "\n",
    "    def fast_cleaning(self, batch_no, data):\n",
    "        '''Method using multiple cores to clean the text.'''\n",
    "        def init_worker(mps, fps, cut):\n",
    "            memorizedPaths, filepaths, cutoff = mps, fps, cut\n",
    "            DG = 1\n",
    "        def clean(text):\n",
    "            import contractions\n",
    "            from nltk.stem import WordNetLemmatizer\n",
    "            from nltk.corpus import stopwords\n",
    "            import re\n",
    "            import nltk\n",
    "            stop_words = stopwords.words('english')\n",
    "            final_str = []\n",
    "            regular_ex = r'[^a-zA-Z0-9\\s\\.]'\n",
    "            regular_ex_1 = r'[$\\n]'\n",
    "            text = text.lower()\n",
    "            text = re.sub(regular_ex,'',text)\n",
    "            text = re.sub(regular_ex_1,'',text)\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            tokenization = nltk.word_tokenize(text)\n",
    "            for w in tokenization:\n",
    "                w = contractions.fix(w)\n",
    "                if w not in stop_words:\n",
    "                    final_str.append(lemmatizer.lemmatize(w))\n",
    "            return ' '.join(final_str)\n",
    "        try:\n",
    "            result = Parallel(n_jobs=-1, prefer=\"processes\", verbose=6)(\n",
    "            delayed(clean)(i) for i in tqdm(data))\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8408426-7e77-4ae4-bee5-7608b1be88ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m data_cleaner \u001b[38;5;241m=\u001b[39m EnglishDataCleaning(data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTransformers Implementation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLanguage Model\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124menwiki20201020\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      2\u001b[0m                                     save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTransformers Implementation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLanguage Model\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mClean Project\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcleaned_data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mdata_cleaner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [8], line 22\u001b[0m, in \u001b[0;36mEnglishDataCleaning.clean_fast\u001b[1;34m(self, batch_size, start_batch_no)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_fast\u001b[39m(\u001b[38;5;28mself\u001b[39m,batch_size,start_batch_no\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124;03m'''Method handling data in batches.'''\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfig.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     23\u001b[0m         config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLast_batch_cleaned\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir)):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config.json'"
     ]
    }
   ],
   "source": [
    "data_cleaner = EnglishDataCleaning(data_dir='D:\\Transformers Implementation\\Language Model\\Data\\enwiki20201020',\n",
    "                                    save_dir='D:\\Transformers Implementation\\Language Model\\Clean Project\\cleaned_data')\n",
    "data_cleaner.clean_fast(batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f03c900-c09b-4d7c-a949-04d3b2869247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1559eda-2f43-47a2-acad-8a3ba8b859e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "132ef888-0af1-4baf-a6c2-5fca28b98908",
   "metadata": {},
   "source": [
    "#### 2. Making data that will go into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93db8e46-76a9-4feb-85e1-407a9e488e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import joblib\n",
    "class DataPreprocessing:\n",
    "    \n",
    "    def __init__(self, data_dir, save_dir, vocab_path, batch_size):\n",
    "        self.data_dir = data_dir\n",
    "        self.save_dir = save_dir\n",
    "        self.vocab_path = vocab_path\n",
    "        self.vocab = None\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def fast_make_save_sequences(self):\n",
    "        with open('config.json', 'r') as f:\n",
    "            config = json.load(f)\n",
    "        list_of_files = os.listdir(self.data_dir)\n",
    "        if not os.path.isdir(self.save_dir):\n",
    "            os.mkdir(self.save_dir)\n",
    "        print(\"Making voabulary...\")\n",
    "        with open(f'{self.vocab_path}', 'r') as f:\n",
    "            vocab_ = f.read()\n",
    "        vocab = {}\n",
    "        for i in tqdm(vocab_.split()):\n",
    "            if i not in vocab.keys():\n",
    "                vocab[i] = len(vocab)\n",
    "        self.vocab = vocab\n",
    "        del vocab\n",
    "        print(\"Making sequences...\")\n",
    "        self.fast_sequencing(list_of_files[config['last_batch_preprocessed']:config['last_batch_preprocessed']+self.batch_size])\n",
    "        with open('config.json', 'w') as f:\n",
    "            config['last_batch_preprocessed'] = config['last_batch_preprocessed']+self.batch_size\n",
    "            json.dump(config,f)\n",
    "        \n",
    "    def fast_sequencing(self,list_of_files):\n",
    "        manager = multiprocessing.Manager()\n",
    "        done_files = manager.list()\n",
    "        def make_sequence(file):\n",
    "            import os\n",
    "            import pickle\n",
    "            sequences = []\n",
    "            with open(self.data_dir + f'/{file}', 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                for m,j in (enumerate(data)):\n",
    "                    t = []\n",
    "                    for k in j.split():\n",
    "                        if k in self.vocab.keys():\n",
    "                            t.append(self.vocab[k])\n",
    "                    sequences.append(t)\n",
    "            \n",
    "            done_files.append((f'{file}',sequences))\n",
    "            if len(done_files) == self.batch_size:\n",
    "                print(\"Started saving...\")\n",
    "                for i in tqdm(done_files):\n",
    "                    with open(self.save_dir + f'sequences_batch_{i[0]}', 'wb') as f:\n",
    "                        pickle.dump(i[1], f)\n",
    "                done_files[:] = []\n",
    "                print(\"Data saved successfully!\")\n",
    "\n",
    "        try:\n",
    "            result = Parallel(n_jobs=-1, prefer=\"processes\", verbose=6)(\n",
    "            delayed(make_sequence)(i) for i in tqdm(list_of_files))\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01d51568-6c0b-41a7-a637-ab45274d4302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making voabulary...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2655fc98ceec406cab8204b0fb84e104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30522 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making sequences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccecb1a0933453c85cbb3d6fc5e89a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarted saving data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   18.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   18.2s finished\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9582e696ab6468c82e90c4264faa0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "data_preprocessor = DataPreprocessing(data_dir='D:\\Transformers Implementation\\Language Model\\Clean Project\\cleaned_data',\n",
    "                                     save_dir='D:/Transformers Implementation/Language Model/Clean Project/sequences/',\n",
    "                                     vocab_path='D:\\Transformers Implementation\\Language Model\\\\bert_vocab_uncased.txt',batch_size=1)\n",
    "data_preprocessor.fast_make_save_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76db8ade-3178-47e1-807a-90eca543a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CustomPreprocessor:\n",
    "    \n",
    "    def __init__(self, data_dir, save_dir, vocab_path, mask_rate, seq_len, max_mask_per_seq, smallest_len_seq, batch_size):\n",
    "        self.data_dir = data_dir\n",
    "        self.save_dir = save_dir\n",
    "        self.vocab_path = vocab_path\n",
    "        self.mask_rate = mask_rate\n",
    "        self.seq_len = seq_len\n",
    "        self.max_mask_per_seq = max_mask_per_seq\n",
    "        self.smallest_len_seq = smallest_len_seq\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        with open(f'{self.vocab_path}', 'r') as f:\n",
    "            vocab_ = f.read()\n",
    "        vocab = {}\n",
    "        for i in tqdm(vocab_.split()):\n",
    "            if i not in vocab.keys():\n",
    "                vocab[i] = len(vocab)\n",
    "        self.mask_id = vocab['[MASK]']\n",
    "        del vocab\n",
    "        \n",
    "    def fast_make_save_MLM_dataset(self):\n",
    "        list_of_files = os.listdir(self.data_dir)\n",
    "        with open('config.json', 'r') as f:\n",
    "            config = json.load(f)\n",
    "        list_of_files = list_of_files[config['last_batch_custom_preprocessed']:config['last_batch_custom_preprocessed']+self.batch_size]\n",
    "        if not os.path.isdir(self.save_dir):\n",
    "            os.mkdir(self.save_dir)\n",
    "        self.make_MLM_dataset(list_of_files)\n",
    "        with open('config.json', 'w') as f:\n",
    "            config['last_batch_custom_preprocessed'] = config['last_batch_custom_preprocessed']+self.batch_size\n",
    "            json.dump(config, f)\n",
    "        \n",
    "    def make_MLM_dataset(self,list_of_files):\n",
    "        manager = multiprocessing.Manager()\n",
    "        done_files = manager.list()\n",
    "        \n",
    "        def make_MLM(file):\n",
    "            import random\n",
    "            import pickle\n",
    "            print(file)\n",
    "            with open(self.data_dir + f'{file}', 'rb') as f:\n",
    "                sequences = pickle.load(f)\n",
    "            \n",
    "            mask_positions = []\n",
    "            target_values = []\n",
    "            weights = []\n",
    "            new_sequences = []\n",
    "            for m,i in tqdm(enumerate(sequences)):\n",
    "                if len(i) > self.smallest_len_seq:\n",
    "                    t = []\n",
    "                    t_1 = []\n",
    "                    t_2 = []\n",
    "                    for k,j in enumerate(i):\n",
    "                        if k < self.seq_len:\n",
    "                            if random.random() <= self.mask_rate:\n",
    "                                if len(t) < self.max_mask_per_seq:\n",
    "                                    t.append(k)\n",
    "                                    t_1.append(j)\n",
    "                                    t_2.append(1)\n",
    "                                    i[k] = self.mask_id\n",
    "                    weights.append(t_2 if len(t_2) == self.max_mask_per_seq else (t_2 + [0]*(self.max_mask_per_seq-len(t_2))))\n",
    "                    mask_positions.append(t if len(t) == self.max_mask_per_seq else (t+[0]*(self.max_mask_per_seq-len(t_2))))\n",
    "                    target_values.append(t_1 if len(t_1) == self.max_mask_per_seq else (t_1+[0]*(self.max_mask_per_seq-len(t_1))))\n",
    "                    new_sequences.append(i[:self.seq_len]+[0]*(self.seq_len-len(i[:self.seq_len])))\n",
    "\n",
    "            done_files.append((f'{file}',({'tokens':tf.convert_to_tensor(new_sequences),'mask_positions':tf.convert_to_tensor(mask_positions,dtype='int32')},\n",
    "            tf.convert_to_tensor(target_values), tf.convert_to_tensor(weights))))\n",
    "            print(\"file name is: \",file)\n",
    "            if len(done_files) == self.batch_size:\n",
    "                print(\"Sarted saving MLM data...\")\n",
    "                for i in tqdm(done_files):\n",
    "                    with open(self.save_dir + f'mlm_batch_{i[0]}', 'wb') as f:\n",
    "                        pickle.dump(i[1], f)\n",
    "                done_files[:] = []\n",
    "                print(\"MLM Data saved successfully!\")\n",
    "        try:\n",
    "            print(\"Making dataset for MLM modeling...\")\n",
    "            result = Parallel(n_jobs=-1, prefer=\"processes\", verbose=6)(\n",
    "            delayed(make_MLM)(i) for i in tqdm(list_of_files))\n",
    "            print(\"Done...\")\n",
    "        except Exception as e:\n",
    "            print(\"Exception is in MLM: \",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e3dcb7f-bb83-40d2-8646-2fd3fe88d14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f490f1644f44fc2bd33f80960733873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30522 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e1c59d877b46aa8194825114ff2b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   47.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   47.5s finished\n"
     ]
    }
   ],
   "source": [
    "custom = CustomPreprocessor(data_dir='D:/Transformers Implementation/Language Model/Clean Project/sequences/',\n",
    "                           save_dir='D:/Transformers Implementation/Language Model/Clean Project/mlm/',\n",
    "                            vocab_path='D:\\Transformers Implementation\\Language Model\\\\bert_vocab_uncased.txt',\n",
    "                           mask_rate=0.25,\n",
    "                           seq_len=20,\n",
    "                           max_mask_per_seq=3,smallest_len_seq=5,\n",
    "                           batch_size = 2)\n",
    "custom.fast_make_save_MLM_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "869af2c6-edcf-420c-b8b2-dc5a2ceadf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import keras\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorboard.plugins import projector\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import time\n",
    "import json\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, log_dir, vocab, encoder_model):\n",
    "        self.log_dir = log_dir\n",
    "        self.vocab = vocab\n",
    "        self.encoder_model = encoder_model\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(\"Logging data...\")\n",
    "        self.log_tensorboard_projector_data()\n",
    "        print(\"Starting tensorboard. Please wait for 10 to 15 seconds!!\")\n",
    "        self.tensorboard_reload()\n",
    "        time.sleep(5)\n",
    "        print(\"Tensorboard started.\")\n",
    "        \n",
    "    def tensorboard_reload(self):\n",
    "        print(\"Reloading tensorboard...\")\n",
    "        os.system('taskkill /IM \"tensorboard.exe\" /F')\n",
    "        print(\"Please wait for 10 to 15 seconds!\")\n",
    "        pid = subprocess.Popen(['tensorboard', f'''--logdir={self.log_dir}'''])\n",
    "    \n",
    "    def log_tensorboard_projector_data(self):\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "\n",
    "        with open(os.path.join(self.log_dir, 'metadata.tsv'), \"w\") as f:\n",
    "            for subwords in self.vocab.keys():\n",
    "                f.write(\"{}\\n\".format(subwords))\n",
    "\n",
    "            for unknown in range(1, len(self.vocab) - len(self.vocab)):\n",
    "                f.write(\"unknown #{}\\n\".format(unknown))\n",
    "        \n",
    "        weights = tf.Variable(self.encoder_model.layers[1].get_weights()[0])\n",
    "\n",
    "        checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "        checkpoint.save(os.path.join(self.log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "\n",
    "        config = projector.ProjectorConfig()\n",
    "        embedding = config.embeddings.add()\n",
    "\n",
    "        embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "        embedding.metadata_path = 'metadata.tsv'\n",
    "        projector.visualize_embeddings(self.log_dir, config)\n",
    "\n",
    "class LanguageModel():\n",
    "\n",
    "    def __init__(self, seq_len, vocab_path, embedding_dim, num_layers,\n",
    "                 intermediate_dim, num_heads, dropout, norm_epsilon, learning_rate, max_mask_per_seq, log_dir,\n",
    "                transfer_learning_batch, models_save_path, model_checkpoint):\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_path = vocab_path\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.norm_epsilon = norm_epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_mask_per_seq = max_mask_per_seq\n",
    "        self.log_dir = log_dir\n",
    "        self.transfer_learning_batch = transfer_learning_batch\n",
    "        self.models_save_path = models_save_path\n",
    "        self.model_checkpoint = model_checkpoint\n",
    "        self.encoder = None\n",
    "        self.bert = None\n",
    "\n",
    "\n",
    "        with open(self.vocab_path, 'r') as f:\n",
    "            vocab_ = f.read()\n",
    "        vocab = {}\n",
    "        for i in (vocab_.split()):\n",
    "            if i not in vocab.keys():\n",
    "                vocab[i] = len(vocab)\n",
    "        self.vocab = vocab\n",
    "        del vocab\n",
    "        \n",
    "    def make_bert(self):\n",
    "        inputs = keras.Input(shape=(self.seq_len,), dtype=tf.int32)\n",
    "        embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "        vocabulary_size=len(self.vocab),\n",
    "        sequence_length=self.seq_len,\n",
    "        embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "\n",
    "        outputs = embedding_layer(inputs)\n",
    "        outputs = keras.layers.LayerNormalization(epsilon=self.norm_epsilon)(outputs)\n",
    "        outputs = keras.layers.Dropout(rate=self.dropout)(outputs)\n",
    "        for i in range(1):\n",
    "            outputs = keras_nlp.layers.TransformerEncoder(\n",
    "            intermediate_dim=self.intermediate_dim,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout=self.dropout,\n",
    "            layer_norm_epsilon=self.norm_epsilon,\n",
    "        )(outputs)\n",
    "\n",
    "        encoder_model = keras.Model(inputs, outputs)\n",
    "        self.encoder_model = encoder_model\n",
    "\n",
    "        encoder_model.summary()\n",
    "        inputs = {\n",
    "        \"tokens\": keras.Input(shape=(self.seq_len,), dtype=tf.int32),\n",
    "        \"mask_positions\": keras.Input(shape=(self.max_mask_per_seq,), dtype=tf.int32),\n",
    "        }\n",
    "        encoded_tokens = encoder_model(inputs[\"tokens\"])\n",
    "        outputs = keras_nlp.layers.MLMHead(\n",
    "        embedding_weights=embedding_layer.token_embedding.embeddings, activation=\"softmax\",\n",
    "        )(encoded_tokens, mask_positions=inputs[\"mask_positions\"])\n",
    "        bert = keras.Model(inputs, outputs)\n",
    "        bert.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n",
    "        weighted_metrics=[\"sparse_categorical_accuracy\"],\n",
    "        jit_compile=True,\n",
    "        )\n",
    "        bert.summary()\n",
    "        self.bert = bert\n",
    "    \n",
    "    def fit(self, model, train_ds, epochs, batch_size, start_batch_no=1):\n",
    "        \n",
    "        if os.path.exists('config.json'):\n",
    "            with open('config.json', 'r') as f:\n",
    "                config = json.load(f)\n",
    "                start_batch_no = config['last_trained_batch']\n",
    "    \n",
    "        list_of_files = os.listdir(train_ds)[start_batch_no:start_batch_no+self.transfer_learning_batch]\n",
    "        with open('config.json', 'w') as f:\n",
    "            config['last_trained_batch'] = start_batch_no+self.transfer_learning_batch\n",
    "            json.dump(config,f)\n",
    "            \n",
    " \n",
    "        tensorboard = TensorBoard(log_dir=self.log_dir, histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)\n",
    "        if start_batch_no <= len(list_of_files)//batch_size+1:\n",
    "            \n",
    "            total_transfer_learning_steps = len(list_of_files)//self.transfer_learning_batch\n",
    "            remaining_files = list_of_files[total_transfer_learning_steps*self.transfer_learning_batch:]\n",
    "            \n",
    "            if not os.path.exists(self.model_checkpoint):\n",
    "                os.mkdir(self.model_checkpoint) \n",
    "                \n",
    "            for i in range(list_of_files):\n",
    "                if not os.path.exists(self.model_checkpoint + f'/{start_batch_no+i}'):\n",
    "                    os.mkdir(self.model_checkpoint + f'/{start_batch_no+i}')\n",
    "                \n",
    "                check_point = ModelCheckpoint(self.model_checkpoint + f'/{start_batch_no+i}',monitor='sparse_categorical_accuracy',mode='max')\n",
    "                \n",
    "                print(f\"Starting training of batch {start_batch_no+1}...\")\n",
    "                data = []\n",
    "                \n",
    "                for j in (cur_files):\n",
    "                    with open(train_ds + f\"/{j}\", 'rb') as f:\n",
    "                        data.append(pickle.load(f))\n",
    "                        \n",
    "                        \n",
    "                training_data = ({\"tokens\":tf.convert_to_tensor(tf.concat([d[0]['tokens'] for d in data],0)),\n",
    "                                 \"mask_positions\":tf.convert_to_tensor(tf.concat([d[0]['mask_positions'] for d in data],0))},\n",
    "                                tf.convert_to_tensor(tf.concat([d[1] for d in data],0)),\n",
    "                                tf.convert_to_tensor(tf.concat([d[2] for d in data],0)))\n",
    "                \n",
    "                \n",
    "                    \n",
    "                self.bert.fit(training_data[0],(training_data[1],training_data[2]), epochs=epochs, batch_size=batch_size, \n",
    "                              callbacks=[CustomCallback(self.log_dir,self.vocab,self.encoder_model),tensorboard,check_point])\n",
    "                print(\"saving the model...\")\n",
    "                self.bert.save(self.models_save_path + f'model_transfer_step_{start_batch_no+1}_with_epochs_{epochs}')\n",
    "                print(f\"Training of batch {start_batch_no+1} done. now procedding to the next transfer learning step.\")\n",
    "        else:\n",
    "            print(\"Error! start_batch_no should be >= 1\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e348863-c58d-4b03-8772-d8f29eac62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_handler = LanguageModel(seq_len=20,\n",
    "                                      vocab_path='D:\\Transformers Implementation\\Language Model\\\\bert_vocab_uncased.txt',\n",
    "                                      embedding_dim=256,\n",
    "                                      num_layers=1,\n",
    "                                      num_heads=4,\n",
    "                                    intermediate_dim=256,\n",
    "                                     dropout=0.1,\n",
    "                                      norm_epsilon=1e-5,\n",
    "                                      learning_rate=5e-4,\n",
    "                                      max_mask_per_seq=3,\n",
    "                                      log_dir='D:/Transformers Implementation/Language Model/Clean Project/logs/language_model/',\n",
    "                                      transfer_learning_batch=1,\n",
    "                                      models_save_path='D:/Transformers Implementation/Language Model/Clean Project/models/',\n",
    "                                      model_checkpoint='D:/Transformers Implementation/Language Model/Clean Project/model_checkpoints/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9477da47-5755-4edb-bb42-3879e6df1982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Transformers Implementation\\Language Model\\language_model\\lib\\site-packages\\keras\\initializers\\initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 20, 256)          7818752   \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " layer_normalization (LayerN  (None, 20, 256)          512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20, 256)           0         \n",
      "                                                                 \n",
      " transformer_encoder (Transf  (None, 20, 256)          395776    \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,215,040\n",
      "Trainable params: 8,215,040\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 20, 256)      8215040     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " mlm_head (MLMHead)             (None, 3, 30522)     7910458     ['model[0][0]',                  \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,311,866\n",
      "Trainable params: 8,311,866\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert = language_model_handler.make_bert()\n",
    "language_model_handler.fit(bert,train_ds='D:/Transformers Implementation/Language Model/Clean Project/mlm/',\n",
    "                          epochs=10,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d72672-4be9-47f9-8318-4dd07a96eaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_handler.make_bert()\n",
    "language_model_handler.log_tensorboard_projector_data()\n",
    "language_model_handler.tensorboard_reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5be50faf-1fb2-44f6-a83a-eced9e74caf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def tensorboard_reload():\n",
    "    print(os.system('taskkill /IM \"tensorboard.exe\" /F'))\n",
    "    # print(\"Starting tensorboard...\")\n",
    "    # pid = subprocess.Popen([sys.executable, f'''tensorboard --logdir=\"D:/Transformers Implementation/Language Model/Clean Project/logs/language_model\"''']) # Call subprocess\n",
    "    # print(\"done... \",pid)\n",
    "tensorboard_reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17139172-a534-423b-a7b2-9f77d8326c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41836b7d-6d9d-4a6b-ba47-3ee97676a81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1d66a7729e89850b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1d66a7729e89850b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=\"D:/Transformers Implementation/Language Model/Clean Project/logs/language_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fa0a0a8-1795-4c9b-b351-09f4fe23c6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<os._wrap_close at 0x161c3134e80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.popen(f'''tensorboard --logdir=\"D:/Transformers Implementation/Language Model/Clean Project/logs/language_model\"''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d95a12f1-62f1-437b-880b-96a839832737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<subprocess.Popen object at 0x00000161EB7F8B50>\n"
     ]
    }
   ],
   "source": [
    "pid = subprocess.Popen(['tensorboard', f'''--logdir=\"D:/Transformers Implementation/Language Model/Clean Project/logs/language_model\"'''])\n",
    "# Call subprocess\n",
    "print(pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17ce70-f33d-407f-b541-346b04223004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.run(['ls', '-l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c0db314-33a2-4d04-9997-e619eb93ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making final class to handle everything\n",
    "import multiprocessing\n",
    "\n",
    "class Main():\n",
    "    def __init__(self, data_dir, cleaned_data_dir, preprocessed_save_dir,\n",
    "     vocab_path, processing_batch_size, custom_preprocessing_save_dir,\n",
    "     mask_rate, seq_len, max_mask_per_seq, smallest_len_seq, \n",
    "     embedding_dim, num_layers, intermediate_dim, num_heads, dropout, norm_epsilon,\n",
    "     learning_rate, log_dir, transfer_learning_batch, models_save_path, model_checkpoint_path,\n",
    "            epochs, batch_size\n",
    "     ):\n",
    "        self.data_dir = data_dir\n",
    "        self.cleaned_data_dir = cleaned_data_dir\n",
    "        self.processing_batch_size = processing_batch_size\n",
    "        self.preprocessed_data_dir = cleaned_data_dir\n",
    "        self.preprocessed_save_dir = preprocessed_save_dir\n",
    "        self.vocab_path = vocab_path\n",
    "        self.custom_preprocessing_data_dir = preprocessed_save_dir\n",
    "        self.custom_preprocessing_save_dir = custom_preprocessing_save_dir\n",
    "        self.mask_rate = mask_rate\n",
    "        self.seq_len = seq_len\n",
    "        self.max_mask_per_seq = max_mask_per_seq\n",
    "        self.smallest_len_seq = smallest_len_seq\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.norm_epsilon = norm_epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.log_dir = log_dir\n",
    "        self.transfer_learning_batch = transfer_learning_batch\n",
    "        self.models_save_path = models_save_path\n",
    "        self.model_checkpoint_path = model_checkpoint_path\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def cleaning(self):\n",
    "        data_cleaner = EnglishDataCleaning(data_dir=self.data_dir,\n",
    "                                    save_dir=self.cleaned_data_dir)\n",
    "        data_cleaner.clean_fast(batch_size=self.processing_batch_size)\n",
    "        \n",
    "    def preprocessing(self):\n",
    "        data_preprocessor = DataPreprocessing(data_dir=self.preprocessed_data_dir,\n",
    "                                     save_dir=self.preprocessed_save_dir,\n",
    "                                     vocab_path=self.vocab_path,batch_size=self.processing_batch_size)\n",
    "        \n",
    "        data_preprocessor.fast_make_save_sequences()\n",
    "        \n",
    "    def custom_preprocessing(self):\n",
    "        custom_preprocessor = CustomPreprocessor(data_dir=self.custom_preprocessing_data_dir,\n",
    "                           save_dir=self.custom_preprocessing_save_dir,\n",
    "                            vocab_path=self.vocab_path,\n",
    "                           mask_rate=self.mask_rate,\n",
    "                           seq_len=self.seq_len,\n",
    "                           max_mask_per_seq=self.max_mask_per_seq,smallest_len_seq=self.smallest_len_seq,\n",
    "                           batch_size = self.processing_batch_size)\n",
    "        \n",
    "        custom_preprocessor.fast_make_save_MLM_dataset()\n",
    "\n",
    "    def training(self):\n",
    "        print(\"naman...\")\n",
    "        prev = 0\n",
    "        while True:\n",
    "            \n",
    "            if os.listdir(self.custom_preprocessing_save_dir) != prev:\n",
    "                prev = os.listdir(self.custom_preprocessing_save_dir)\n",
    "                language_model_handler = LanguageModel(seq_len=self.seq_len,\n",
    "                                              vocab_path=self.vocab_path,\n",
    "                                              embedding_dim=self.embedding_dim,\n",
    "                                              num_layers=self.num_layers,\n",
    "                                              num_heads=self.num_heads,\n",
    "                                            intermediate_dim=self.intermediate_dim,\n",
    "                                             dropout=self.dropout,\n",
    "                                              norm_epsilon=self.norm_epsilon,\n",
    "                                              learning_rate=self.learning_rate,\n",
    "                                              max_mask_per_seq=self.max_mask_per_seq,\n",
    "                                              log_dir=self.log_dir,\n",
    "                                              transfer_learning_batch=1,\n",
    "                                              models_save_path=self.models_save_path,\n",
    "                                              model_checkpoint=self.model_checkpoint_path)\n",
    "                bert = language_model_handler.make_bert()\n",
    "                language_model_handler.fit(bert,train_ds='D:/Transformers Implementation/Language Model/Clean Project/mlm/',\n",
    "                                  epochs=self.epochs,batch_size=self.batch_size)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    def preprocessing_handler(self):\n",
    "        \n",
    "        if os.path.exists('config.json'):\n",
    "            with open('config.json', 'r') as f:\n",
    "                try:\n",
    "                    config = json.load(f)\n",
    "                except:\n",
    "                    print(\"Try deleting config.json file!\")\n",
    "        else:\n",
    "            config= dict()\n",
    "            config[\"Last_batch_cleaned\"] = 0\n",
    "            config[\"last_batch_preprocessed\"] = 0\n",
    "            config[\"last_batch_custom_preprocessed\"] = 0\n",
    "            config[\"last_trained_batch\"] = 0\n",
    "            print(config)\n",
    "            with open('config.json', 'w') as f:\n",
    "                json.dump(config, f)\n",
    "\n",
    "        list_of_files = os.listdir(self.data_dir)\n",
    "        total_iterations = len(list_of_files)//self.processing_batch_size\n",
    "        for i in range(total_iterations+1):\n",
    "            self.cleaning()\n",
    "            self.preprocessing()\n",
    "            self.custom_preprocessing()\n",
    "\n",
    "    def main(self):\n",
    "#         p1 = multiprocessing.Process(target=self.preprocessing_handler)\n",
    "#         p2 = multiprocessing.Process(target=self.training)\n",
    "    \n",
    "#         p1.start()\n",
    "#         p2.start()\n",
    "        \n",
    "#         p1.join()\n",
    "        # p2.join()\n",
    "        self.preprocessing_handler()\n",
    "        \n",
    "        print(\"naman\")\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4f61b7f-dd88-4f17-846b-7f57f152f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Language_model = Main(data_dir='D:/Transformers Implementation/Language Model/Data/enwiki20201020',\n",
    "           cleaned_data_dir='D:/Transformers Implementation/Language Model/Clean Project/cleaned_data/',\n",
    "           processing_batch_size=1,\n",
    "           preprocessed_save_dir='D:/Transformers Implementation/Language Model/Clean Project/sequences/',\n",
    "           vocab_path = 'D:\\Transformers Implementation\\Language Model\\\\bert_vocab_uncased.txt',\n",
    "           custom_preprocessing_save_dir = 'D:/Transformers Implementation/Language Model/Clean Project/mlm/',\n",
    "           mask_rate = 0.25,\n",
    "           seq_len = 20,\n",
    "           max_mask_per_seq = 3,\n",
    "           smallest_len_seq = 5,\n",
    "           embedding_dim = 256,\n",
    "           num_layers = 1,\n",
    "           intermediate_dim = 512,\n",
    "           num_heads = 4,\n",
    "           dropout = 0.1,\n",
    "           norm_epsilon = 1e-5,\n",
    "           learning_rate = 5e-4,\n",
    "           log_dir = 'D:/Transformers Implementation/Language Model/Clean Project/logs/language_model/',\n",
    "           transfer_learning_batch = 5,\n",
    "           models_save_path = 'D:/Transformers Implementation/Language Model/Clean Project/models/',\n",
    "           model_checkpoint_path = 'D:/Transformers Implementation/Language Model/Clean Project/model_checkpoints/',\n",
    "           epochs = 10,\n",
    "           batch_size = 128\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68aca171-38f6-4071-8266-3daeb6b088ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading batch 1 into memory...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74efb197978b4a41a5859367c0a91370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded batch 1.\n",
      "Cleaning batch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05a0e8c736547e5a9fc5d62ae8e67e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/331352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done 6168 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done 20632 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 38168 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 59032 tasks      | elapsed:   26.8s\n",
      "[Parallel(n_jobs=-1)]: Done 82968 tasks      | elapsed:   33.3s\n",
      "[Parallel(n_jobs=-1)]: Done 110232 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=-1)]: Done 140568 tasks      | elapsed:   48.6s\n",
      "[Parallel(n_jobs=-1)]: Done 174232 tasks      | elapsed:   57.5s\n",
      "[Parallel(n_jobs=-1)]: Done 210968 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 251032 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 294168 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 331352 out of 331352 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning of batch 1 done.\n",
      "Saving batch 1...\n",
      "Batch no 1 saved successfully!\n",
      "Making voabulary...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e025445eead34a30a50fba85ae53996e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30522 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making sequences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a9449a086e4b46b0e92bb331605dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    4.9s finished\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1447d21d8baa4ae59b98bd32a0fa39b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30522 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dataset for MLM modeling...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a277091f3b84b0f900947b09cf1f033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   16.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done...\n",
      "Loading batch 2 into memory...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6567998bc5ad40f584c52616ab2cf177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded batch 2.\n",
      "Cleaning batch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f63aac78234769871afe1751543fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 6704 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 13936 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 25520 tasks      | elapsed:    8.9s\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Language_model.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeb0e09-ad08-47ee-b1a0-005500b4639d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "language_model_kernel",
   "language": "python",
   "name": "language_model_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
