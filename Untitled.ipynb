{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec10faf3-3669-4e32-851d-17e4afc8eeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import keras\n",
    "import keras_nlp\n",
    "import numpy as np\n",
    "from tensorboard.plugins import projector\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import time\n",
    "import json\n",
    "\n",
    "class EnglishDataCleaning:\n",
    "    ''' This class attempts to clean the raw english text by employing multiple cores. This class takes\n",
    "        list of raw english strings as input and using clean_fast method it cleans the english text.\n",
    "        clean_fast method also takes batch_size as parameter to perform cleaning in batches if data\n",
    "        does not fit into the main memeory.\n",
    "        \n",
    "        Note: This method removes stop words.\n",
    "        '''\n",
    "    def __init__(self, data_dir, save_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        \n",
    "    def clean_fast(self,batch_size,start_batch_no=1):\n",
    "        '''Method handling data in batches.'''\n",
    "        list_of_files = os.listdir(self.data_dir)[start_batch_no:]\n",
    "        if start_batch_no >= 1 and start_batch_no <= len(list_of_files)//batch_size+1:\n",
    "            \n",
    "            total_no_of_batches = len(list_of_files)//batch_size\n",
    "            remaining_files = list_of_files[total_no_of_batches*batch_size:]\n",
    "            \n",
    "            for i in range(total_no_of_batches+1):\n",
    "                if i == total_no_of_batches:\n",
    "                    cur_files = remaining_files\n",
    "                else:\n",
    "                    cur_files = list_of_files[i*batch_size:i*batch_size+batch_size]\n",
    "                print(f\"Loading batch {i+1} into memory...\")\n",
    "                data = []\n",
    "                for j in tqdm(cur_files):\n",
    "                    with open(self.data_dir + f\"/{j}\", 'rb') as f:\n",
    "                        for k in json.load(f):\n",
    "                            for m in k['text'].split('.'):\n",
    "                                data.append(m)\n",
    "                print(f'Loaded batch {i+1}.')\n",
    "                print(f'Cleaning batch {i+1}')\n",
    "                self.fast_cleaning(i,data)\n",
    "                print(f'Cleaning of batch {i+1} done.')\n",
    "                print(f'Saving batch {i+1}...')\n",
    "                if not os.path.isdir(self.save_dir):\n",
    "                    os.mkdir(self.save_dir)\n",
    "                with open(f'{self.save_dir}/cleaned_batch_{i+1}.pkl', 'wb') as f:\n",
    "                    pickle.dump(data, f)\n",
    "                print(f'Batch no {i+1} saved successfully!')\n",
    "        else:\n",
    "            print(\"Error! start_batch_no should be >= 1\")\n",
    "\n",
    "    def fast_cleaning(self, batch_no, data):\n",
    "        '''Method using multiple cores to clean the text.'''\n",
    "        def init_worker(mps, fps, cut):\n",
    "            memorizedPaths, filepaths, cutoff = mps, fps, cut\n",
    "            DG = 1\n",
    "        def clean(text):\n",
    "            import contractions\n",
    "            from nltk.stem import WordNetLemmatizer\n",
    "            from nltk.corpus import stopwords\n",
    "            import re\n",
    "            import nltk\n",
    "            stop_words = stopwords.words('english')\n",
    "            final_str = []\n",
    "            regular_ex = r'[^a-zA-Z0-9\\s\\.]'\n",
    "            regular_ex_1 = r'[$\\n]'\n",
    "            text = text.lower()\n",
    "            text = re.sub(regular_ex,'',text)\n",
    "            text = re.sub(regular_ex_1,'',text)\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            tokenization = nltk.word_tokenize(text)\n",
    "            for w in tokenization:\n",
    "                w = contractions.fix(w)\n",
    "                if w not in stop_words:\n",
    "                    final_str.append(lemmatizer.lemmatize(w))\n",
    "            return ' '.join(final_str)\n",
    "        try:\n",
    "            result = Parallel(n_jobs=-1, prefer=\"processes\", verbose=6)(\n",
    "            delayed(clean)(i) for i in tqdm(data))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "class DataPreprocessing:\n",
    "    \n",
    "    def __init__(self, data_dir, save_dir, vocab_path, batch_size):\n",
    "        self.data_dir = data_dir\n",
    "        self.save_dir = save_dir\n",
    "        self.vocab_path = vocab_path\n",
    "        self.vocab = None\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def fast_make_save_sequences(self):\n",
    "        list_of_files = os.listdir(self.data_dir)\n",
    "        if not os.path.isdir(self.save_dir):\n",
    "            os.mkdir(self.save_dir)\n",
    "        print(\"Making voabulary...\")\n",
    "        with open(f'{self.vocab_path}', 'r') as f:\n",
    "            vocab_ = f.read()\n",
    "        vocab = {}\n",
    "        for i in tqdm(vocab_.split()):\n",
    "            if i not in vocab.keys():\n",
    "                vocab[i] = len(vocab)\n",
    "        self.vocab = vocab\n",
    "        del vocab\n",
    "        print(\"Making sequences...\")\n",
    "        self.fast_sequencing()\n",
    "        \n",
    "        \n",
    "    def fast_sequencing(self):\n",
    "        manager = multiprocessing.Manager()\n",
    "        done_files = manager.list()\n",
    "        def make_sequence(file):\n",
    "            import os\n",
    "            import pickle\n",
    "            sequences = []\n",
    "            with open(self.data_dir + f'/{file}', 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                for m,j in (enumerate(data)):\n",
    "                    t = []\n",
    "                    for k in j.split():\n",
    "                        if k in self.vocab.keys():\n",
    "                            t.append(self.vocab[k])\n",
    "                    sequences.append(t)\n",
    "            \n",
    "            done_files.append((f'{file}',sequences))\n",
    "            if len(done_files) == batch_size:\n",
    "                print(\"Started saving...\")\n",
    "                for i in tqdm(done_files):\n",
    "                    with open(self.save_dir + f'sequences_batch_{i[0]}', 'wb') as f:\n",
    "                        pickle.dump(i[1], f)\n",
    "                done_files[:] = []\n",
    "                print(\"Data saved successfully!\")\n",
    "\n",
    "        try:\n",
    "            result = Parallel(n_jobs=-1, prefer=\"processes\", verbose=6)(\n",
    "            delayed(make_sequence)(i) for i in tqdm(os.listdir(self.data_dir)))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "class CustomPreprocessor:\n",
    "    \n",
    "    def __init__(self, data_dir, save_dir, vocab_path, mask_rate, seq_len, max_mask_per_seq, smallest_len_seq, batch_size):\n",
    "        self.data_dir = data_dir\n",
    "        self.save_dir = save_dir\n",
    "        self.vocab_path = vocab_path\n",
    "        self.mask_rate = mask_rate\n",
    "        self.seq_len = seq_len\n",
    "        self.max_mask_per_seq = max_mask_per_seq\n",
    "        self.smallest_len_seq = smallest_len_seq\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        with open(f'{self.vocab_path}', 'r') as f:\n",
    "            vocab_ = f.read()\n",
    "        vocab = {}\n",
    "        for i in tqdm(vocab_.split()):\n",
    "            if i not in vocab.keys():\n",
    "                vocab[i] = len(vocab)\n",
    "        self.mask_id = vocab['[MASK]']\n",
    "        del vocab\n",
    "        \n",
    "    def fast_make_save_MLM_dataset(self):\n",
    "        list_of_files = os.listdir(self.data_dir)\n",
    "        if not os.path.isdir(self.save_dir):\n",
    "            os.mkdir(self.save_dir)\n",
    "        self.make_MLM_dataset()\n",
    "        \n",
    "    def make_MLM_dataset(self):\n",
    "        manager = multiprocessing.Manager()\n",
    "        done_files = manager.list()\n",
    "        \n",
    "        def make_MLM(file):\n",
    "            import random\n",
    "            import pickle\n",
    "            with open(self.data_dir + f'{file}', 'rb') as f:\n",
    "                sequences = pickle.load(f)\n",
    "            \n",
    "            mask_positions = []\n",
    "            target_values = []\n",
    "            weights = []\n",
    "            new_sequences = []\n",
    "            for m,i in tqdm(enumerate(sequences)):\n",
    "                if len(i) > self.smallest_len_seq:\n",
    "                    t = []\n",
    "                    t_1 = []\n",
    "                    t_2 = []\n",
    "                    for k,j in enumerate(i):\n",
    "                        if k < self.seq_len:\n",
    "                            if random.random() <= self.mask_rate:\n",
    "                                if len(t) < self.max_mask_per_seq:\n",
    "                                    t.append(k)\n",
    "                                    t_1.append(j)\n",
    "                                    t_2.append(1)\n",
    "                                    i[k] = self.mask_id\n",
    "                    weights.append(t_2 if len(t_2) == self.max_mask_per_seq else (t_2 + [0]*(self.max_mask_per_seq-len(t_2))))\n",
    "                    mask_positions.append(t if len(t) == self.max_mask_per_seq else (t+[0]*(self.max_mask_per_seq-len(t_2))))\n",
    "                    target_values.append(t_1 if len(t_1) == self.max_mask_per_seq else (t_1+[0]*(self.max_mask_per_seq-len(t_1))))\n",
    "                    new_sequences.append(i[:self.seq_len]+[0]*(self.seq_len-len(i[:self.seq_len])))\n",
    "\n",
    "            done_files.append((file[-1],({'tokens':tf.convert_to_tensor(new_sequences),'mask_positions':tf.convert_to_tensor(mask_positions,dtype='int32')},\n",
    "            tf.convert_to_tensor(target_values), tf.convert_to_tensor(weights))))\n",
    "            if len(done_files) == batch_size:\n",
    "                print(\"Sarted saving data...\")\n",
    "                for i in tqdm(done_files):\n",
    "                    with open(self.save_dir + f'sequences_batch_{i[0]}', 'wb') as f:\n",
    "                        pickle.dump(i[1], f)\n",
    "                done_files[:] = []\n",
    "                print(\"Data saved successfully!\")\n",
    "        try:\n",
    "            result = Parallel(n_jobs=-1, prefer=\"processes\", verbose=6)(\n",
    "            delayed(make_MLM)(i) for i in tqdm(os.listdir(self.data_dir)))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, log_dir, vocab, encoder_model):\n",
    "        self.log_dir = log_dir\n",
    "        self.vocab = vocab\n",
    "        self.encoder_model = encoder_model\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(\"Logging data...\")\n",
    "        self.log_tensorboard_projector_data()\n",
    "        print(\"Starting tensorboard. Please wait for 10 to 15 seconds!!\")\n",
    "        self.tensorboard_reload()\n",
    "        time.sleep(5)\n",
    "        print(\"Tensorboard started.\")\n",
    "        \n",
    "    def tensorboard_reload(self):\n",
    "        print(\"Reloading tensorboard...\")\n",
    "        os.system('taskkill /IM \"tensorboard.exe\" /F')\n",
    "        print(\"Please wait for 10 to 15 seconds!\")\n",
    "        pid = subprocess.Popen(['tensorboard', f'''--logdir={self.log_dir}'''])\n",
    "    \n",
    "    def log_tensorboard_projector_data(self):\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "\n",
    "        with open(os.path.join(self.log_dir, 'metadata.tsv'), \"w\") as f:\n",
    "            for subwords in self.vocab.keys():\n",
    "                f.write(\"{}\\n\".format(subwords))\n",
    "\n",
    "            for unknown in range(1, len(self.vocab) - len(self.vocab)):\n",
    "                f.write(\"unknown #{}\\n\".format(unknown))\n",
    "        \n",
    "        weights = tf.Variable(self.encoder_model.layers[1].get_weights()[0])\n",
    "\n",
    "        checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "        checkpoint.save(os.path.join(self.log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "\n",
    "        config = projector.ProjectorConfig()\n",
    "        embedding = config.embeddings.add()\n",
    "\n",
    "        embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "        embedding.metadata_path = 'metadata.tsv'\n",
    "        projector.visualize_embeddings(self.log_dir, config)\n",
    "\n",
    "\n",
    "class LanguageModel():\n",
    "\n",
    "    def __init__(self, seq_len, vocab_path, embedding_dim, num_layers,\n",
    "                 intermediate_dim, num_heads, dropout, norm_epsilon, learning_rate, max_mask_per_seq, log_dir,\n",
    "                transfer_learning_batch, models_save_path, model_checkpoint):\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_path = vocab_path\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.norm_epsilon = norm_epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_mask_per_seq = max_mask_per_seq\n",
    "        self.log_dir = log_dir\n",
    "        self.transfer_learning_batch = transfer_learning_batch\n",
    "        self.models_save_path = models_save_path\n",
    "        self.model_checkpoint = model_checkpoint\n",
    "        self.encoder = None\n",
    "        self.bert = None\n",
    "\n",
    "\n",
    "        with open(self.vocab_path, 'r') as f:\n",
    "            vocab_ = f.read()\n",
    "        vocab = {}\n",
    "        for i in (vocab_.split()):\n",
    "            if i not in vocab.keys():\n",
    "                vocab[i] = len(vocab)\n",
    "        self.vocab = vocab\n",
    "        del vocab\n",
    "        \n",
    "    def make_bert(self):\n",
    "        inputs = keras.Input(shape=(self.seq_len,), dtype=tf.int32)\n",
    "        embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "        vocabulary_size=len(self.vocab),\n",
    "        sequence_length=self.seq_len,\n",
    "        embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "\n",
    "        outputs = embedding_layer(inputs)\n",
    "        outputs = keras.layers.LayerNormalization(epsilon=self.norm_epsilon)(outputs)\n",
    "        outputs = keras.layers.Dropout(rate=self.dropout)(outputs)\n",
    "        for i in range(1):\n",
    "            outputs = keras_nlp.layers.TransformerEncoder(\n",
    "            intermediate_dim=self.intermediate_dim,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout=self.dropout,\n",
    "            layer_norm_epsilon=self.norm_epsilon,\n",
    "        )(outputs)\n",
    "\n",
    "        encoder_model = keras.Model(inputs, outputs)\n",
    "        self.encoder_model = encoder_model\n",
    "\n",
    "        inputs = {\n",
    "        \"tokens\": keras.Input(shape=(self.seq_len,), dtype=tf.int32),\n",
    "        \"mask_positions\": keras.Input(shape=(self.max_mask_per_seq,), dtype=tf.int32),\n",
    "        }\n",
    "        encoded_tokens = encoder_model(inputs[\"tokens\"])\n",
    "        outputs = keras_nlp.layers.MLMHead(\n",
    "        embedding_weights=embedding_layer.token_embedding.embeddings, activation=\"softmax\",\n",
    "        )(encoded_tokens, mask_positions=inputs[\"mask_positions\"])\n",
    "        bert = keras.Model(inputs, outputs)\n",
    "        bert.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n",
    "        weighted_metrics=[\"sparse_categorical_accuracy\"],\n",
    "        jit_compile=True,\n",
    "        )\n",
    "        self.bert = bert\n",
    "    \n",
    "    def fit(self, model, train_ds, epochs, batch_size, start_batch_no=1):\n",
    "        \n",
    "        if os.path.exists('config.json'):\n",
    "            with open('config.json', 'r') as f:\n",
    "                config = json.load(f)\n",
    "                start_batch_no = config['finished_batches']\n",
    "    \n",
    "        list_of_files = os.listdir(train_ds)[start_batch_no-1:]\n",
    "        with open('config.json', 'w') as f:\n",
    "            config = {\"finished_batches\":len(list_of_files)}\n",
    "            json.dump(config,f)\n",
    "            \n",
    " \n",
    "        tensorboard = TensorBoard(log_dir=self.log_dir, histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)\n",
    "        if start_batch_no >= 1 and start_batch_no <= len(list_of_files)//batch_size+1:\n",
    "            \n",
    "            total_transfer_learning_steps = len(list_of_files)//self.transfer_learning_batch\n",
    "            remaining_files = list_of_files[total_transfer_learning_steps*self.transfer_learning_batch:]\n",
    "            \n",
    "            if not os.path.exists(self.model_checkpoint):\n",
    "                os.mkdir(self.model_checkpoint) \n",
    "                \n",
    "            for i in range(total_transfer_learning_steps+1):\n",
    "                if not os.path.exists(self.model_checkpoint + f'/{i}'):\n",
    "                    os.mkdir(self.model_checkpoint + f'/{i}')\n",
    "                \n",
    "                check_point = ModelCheckpoint(self.model_checkpoint + f'/{i}',monitor='sparse_categorical_accuracy',mode='max')\n",
    "                if i == total_transfer_learning_steps:\n",
    "                    cur_files = remaining_files\n",
    "                else:\n",
    "                    cur_files = list_of_files[i*self.transfer_learning_batch:i*self.transfer_learning_batch+self.transfer_learning_batch]\n",
    "                print(f\"Starting training of batch {i+1}...\")\n",
    "                data = []\n",
    "                \n",
    "                for j in (cur_files):\n",
    "                    with open(train_ds + f\"/{j}\", 'rb') as f:\n",
    "                        data.append(pickle.load(f))\n",
    "                        \n",
    "                        \n",
    "                training_data = ({\"tokens\":tf.convert_to_tensor(tf.concat([d[0]['tokens'] for d in data],0)),\n",
    "                                 \"mask_positions\":tf.convert_to_tensor(tf.concat([d[0]['mask_positions'] for d in data],0))},\n",
    "                                tf.convert_to_tensor(tf.concat([d[1] for d in data],0)),\n",
    "                                tf.convert_to_tensor(tf.concat([d[2] for d in data],0)))\n",
    "                \n",
    "                \n",
    "                    \n",
    "                self.bert.fit(training_data[0],(training_data[1],training_data[2]), epochs=epochs, batch_size=batch_size, \n",
    "                              callbacks=[CustomCallback(self.log_dir,self.vocab,self.encoder_model),tensorboard,check_point])\n",
    "                print(\"saving the model...\")\n",
    "                self.bert.save(self.models_save_path + f'model_transfer_step_{i+1}_with_epochs_{epochs}')\n",
    "                print(f\"Training of batch {i+1} done. now procedding to the next transfer learning step.\")\n",
    "        else:\n",
    "            print(\"Error! start_batch_no should be >= 1\")    \n",
    "\n",
    "\n",
    "\n",
    "class Main():\n",
    "    def __init__(self, data_dir, cleaned_data_dir, cleaning_batch_size, preprocessed_save_dir,\n",
    "     vocab_path, preprocessing_batch_size, custom_preprocessing_save_dir,\n",
    "     mask_rate, seq_len, max_mask_per_seq, smallest_len_seq, custom_preprocessing_batch_size,\n",
    "     embedding_dim, num_layers, intermediate_dim, num_heads, dropout, norm_epsilon,\n",
    "     learning_rate, log_dir, transfer_learning_batch, models_save_path, model_checkpoint_path,\n",
    "            epochs, batch_size\n",
    "     ):\n",
    "        self.data_dir = data_dir\n",
    "        self.cleaned_data_dir = cleaned_data_dir\n",
    "        self.cleaning_batch_size = cleaning_batch_size\n",
    "        self.preprocessed_data_dir = cleaned_data_dir\n",
    "        self.preprocessed_save_dir = preprocessed_save_dir\n",
    "        self.vocab_path = vocab_path\n",
    "        self.preprocessing_batch_size = preprocessing_batch_size\n",
    "        self.custom_preprocessing_data_dir = preprocessed_save_dir\n",
    "        self.custom_preprocessing_save_dir = custom_preprocessing_save_dir\n",
    "        self.mask_rate = mask_rate\n",
    "        self.seq_len = seq_len\n",
    "        self.max_mask_per_seq = max_mask_per_seq\n",
    "        self.smallest_len_seq = smallest_len_seq\n",
    "        self.custom_preprocessing_batch_size =custom_preprocessing_batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.norm_epsilon = norm_epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.log_dir = log_dir\n",
    "        self.transfer_learning_batch = transfer_learning_batch\n",
    "        self.models_save_path = models_save_path\n",
    "        self.model_checkpoint_path = model_checkpoint_path\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def cleaning(self):\n",
    "        data_cleaner = EnglishDataCleaning(data_dir=self.data_dir,\n",
    "                                    save_dir=self.cleaned_data_dir)\n",
    "        data_cleaner.clean_fast(batch_size=self.cleaning_batch_size)\n",
    "        \n",
    "    def preprocessing(self):\n",
    "        data_preprocessor = DataPreprocessing(data_dir=self.preprocessed_data_dir,\n",
    "                                     save_dir=self.preprocessed_save_dir,\n",
    "                                     vocab_path=self.vocab_path,batch_size=self.preprocessing_batch_size)\n",
    "        \n",
    "        data_preprocessor.fast_make_save_sequences()\n",
    "        \n",
    "    def custom_preprocessing(self):\n",
    "        custom_preprocessor = CustomPreprocessor(data_dir=self.custom_preprocessing_data_dira,\n",
    "                           save_dir=self.custom_preprocessing_save_dir,\n",
    "                            vocab_path=self.vocab_path,\n",
    "                           mask_rate=self.mask_rate,\n",
    "                           seq_len=self.seq_len,\n",
    "                           max_mask_per_seq=self.max_mask_per_seq,smallest_len_seq=self.smallest_len_seq,\n",
    "                           batch_size = self.custom_preprocessing_batch_size)\n",
    "        \n",
    "        custom_preprocessor.fast_make_save_MLM_dataset()\n",
    "\n",
    "    def training(self):\n",
    "        prev = 0\n",
    "        while True:\n",
    "            \n",
    "            if os.listdir(self.custom_preprocessing_save_dir) != prev:\n",
    "                prev = os.listdir(self.custom_preprocessing_save_dir)\n",
    "                language_model_handler = LanguageModel(seq_len=self.seq_len,\n",
    "                                              vocab_path=self.vocab_path,\n",
    "                                              embedding_dim=self.embedding_dim,\n",
    "                                              num_layers=self.num_layers,\n",
    "                                              num_heads=self.num_heads,\n",
    "                                            intermediate_dim=self.intermediate_dim,\n",
    "                                             dropout=self.dropout,\n",
    "                                              norm_epsilon=self.norm_epsilon,\n",
    "                                              learning_rate=self.learning_rate,\n",
    "                                              max_mask_per_seq=self.max_mask_per_seq,\n",
    "                                              log_dir=self.log_dir,\n",
    "                                              transfer_learning_batch=1,\n",
    "                                              models_save_path=self.models_save_path,\n",
    "                                              model_checkpoint=self.model_checkpoint_path)\n",
    "                bert = language_model_handler.make_bert()\n",
    "                language_model_handler.fit(bert,train_ds='D:/Transformers Implementation/Language Model/Clean Project/mlm/',\n",
    "                                  epochs=self.epochs,batch_size=self.batch_size)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    def preprocessing_handler(self):\n",
    "        self.cleaning()\n",
    "        self.preprocessing()\n",
    "        self.custom_preprocessing()\n",
    "    \n",
    "    def main(self):\n",
    "        print(os.path.exists(self.cleaned_data_dir))\n",
    "        if not os.path.exists(self.cleaned_data_dir):\n",
    "            os.mkdir(self.cleaned_data_dir)\n",
    "\n",
    "        if not os.path.exists(self.preprocessed_save_dir):\n",
    "            os.mkdir(self.preprocessed_save_dir)\n",
    "\n",
    "        if not os.path.exists(self.custom_preprocessing_save_dir):\n",
    "            os.mkdir(self.custom_preprocessing_save_dir)\n",
    "\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.mkdir(self.log_dir)\n",
    "\n",
    "        if not os.path.exists(self.models_save_path):\n",
    "            os.mkdir(self.models_save_path)\n",
    "\n",
    "        if not os.path.exists(self.model_checkpoint_path):\n",
    "            os.mkdir(self.model_checkpoint_path)\n",
    "            \n",
    "        p1 = multiprocessing.Process(target=self.preprocessing_handler)\n",
    "        p2 = multiprocessing.Process(target=self.training)\n",
    "    \n",
    "        p1.start()\n",
    "        p2.start()\n",
    "\n",
    "Language_model = Main(data_dir='D:/Transformers Implementation/Language Model/Data/enwiki20201020',\n",
    "           cleaned_data_dir='D:/Transformers Implementation/Language Model/Clean Project/cleaned_data/',\n",
    "           cleaning_batch_size=5,\n",
    "           preprocessed_save_dir='D:/Transformers Implementation/Language Model/Clean Project/sequences/',\n",
    "           vocab_path = 'D:\\Transformers Implementation\\Language Model\\\\bert_vocab_uncased.txt',\n",
    "           preprocessing_batch_size = 1,\n",
    "           custom_preprocessing_save_dir = 'D:/Transformers Implementation/Language Model/Clean Project/mlm/',\n",
    "           mask_rate = 0.25,\n",
    "           seq_len = 20,\n",
    "           max_mask_per_seq = 3,\n",
    "           smallest_len_seq = 5,\n",
    "           custom_preprocessing_batch_size = 2,\n",
    "           embedding_dim = 256,\n",
    "           num_layers = 1,\n",
    "           intermediate_dim = 512,\n",
    "           num_heads = 4,\n",
    "           dropout = 0.1,\n",
    "           norm_epsilon = 1e-5,\n",
    "           learning_rate = 5e-4,\n",
    "           log_dir = 'D:/Transformers Implementation/Language Model/Clean Project/logs/',\n",
    "           transfer_learning_batch = 5,\n",
    "           models_save_path = 'D:/Transformers Implementation/Language Model/Clean Project/models/',\n",
    "           model_checkpoint_path = 'D:/Transformers Implementation/Language Model/Clean Project/model_checkpoints/',\n",
    "           epochs = 10,\n",
    "           batch_size = 128\n",
    "           )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Language_model.main()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf51fad-c08e-4c57-8c62-ee39f6b75b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "language_model_kernel",
   "language": "python",
   "name": "language_model_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
